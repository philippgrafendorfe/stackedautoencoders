{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard\n",
    "from __future__ import print_function\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pydot\n",
    "import graphviz\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere. \"Bad\" returns are those that do not; their signals pass through the ionosphere. \n",
    "\n",
    "Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All 34 are continuous \n",
    "- The 35th attribute is either \"good\" or \"bad\" according to the definition summarized above. This is a binary classification task. \n",
    "- https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"data/ionosphere.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1        2        3        4        5        6        7        8   \\\n",
       "0   1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
       "1   1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
       "2   1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
       "3   1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
       "4   1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
       "\n",
       "        9  ...       25       26       27       28       29       30       31  \\\n",
       "0  0.03760 ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267 -0.54487   \n",
       "1 -0.04549 ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626 -0.06288   \n",
       "2  0.01198 ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436 -0.24180   \n",
       "3  0.00000 ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682  1.00000   \n",
       "4 -0.16399 ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707 -0.59573   \n",
       "\n",
       "        32       33  34  \n",
       "0  0.18641 -0.45300   g  \n",
       "1 -0.13738 -0.02447   b  \n",
       "2  0.56045 -0.38238   g  \n",
       "3 -0.32382  1.00000   b  \n",
       "4 -0.04608 -0.65697   g  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/ionosphere.data', sep=\",\", header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample the dataframe\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_x = data.iloc[:,:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = data.iloc[:,34]\n",
    "df_y = [1 if e is \"g\" else 0 for e in df_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6410256410256411"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_y)/len(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the data normalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.0</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.891738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.641342</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>0.601068</td>\n",
       "      <td>0.115889</td>\n",
       "      <td>0.550095</td>\n",
       "      <td>0.119360</td>\n",
       "      <td>0.511848</td>\n",
       "      <td>0.181345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396135</td>\n",
       "      <td>-0.071187</td>\n",
       "      <td>0.541641</td>\n",
       "      <td>-0.069538</td>\n",
       "      <td>0.378445</td>\n",
       "      <td>-0.027907</td>\n",
       "      <td>0.352514</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>0.349364</td>\n",
       "      <td>0.014480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.311155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497708</td>\n",
       "      <td>0.441435</td>\n",
       "      <td>0.519862</td>\n",
       "      <td>0.460810</td>\n",
       "      <td>0.492654</td>\n",
       "      <td>0.520750</td>\n",
       "      <td>0.507066</td>\n",
       "      <td>0.483851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578451</td>\n",
       "      <td>0.508495</td>\n",
       "      <td>0.516205</td>\n",
       "      <td>0.550025</td>\n",
       "      <td>0.575886</td>\n",
       "      <td>0.507974</td>\n",
       "      <td>0.571483</td>\n",
       "      <td>0.513574</td>\n",
       "      <td>0.522663</td>\n",
       "      <td>0.468337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472135</td>\n",
       "      <td>-0.064735</td>\n",
       "      <td>0.412660</td>\n",
       "      <td>-0.024795</td>\n",
       "      <td>0.211310</td>\n",
       "      <td>-0.054840</td>\n",
       "      <td>0.087110</td>\n",
       "      <td>-0.048075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.332390</td>\n",
       "      <td>0.286435</td>\n",
       "      <td>-0.443165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.236885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.242595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.165350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.871110</td>\n",
       "      <td>0.016310</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.728730</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.684210</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553890</td>\n",
       "      <td>-0.015050</td>\n",
       "      <td>0.708240</td>\n",
       "      <td>-0.017690</td>\n",
       "      <td>0.496640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409560</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334655</td>\n",
       "      <td>0.969240</td>\n",
       "      <td>0.445675</td>\n",
       "      <td>0.953240</td>\n",
       "      <td>0.534195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905240</td>\n",
       "      <td>0.156765</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.153535</td>\n",
       "      <td>0.883465</td>\n",
       "      <td>0.154075</td>\n",
       "      <td>0.857620</td>\n",
       "      <td>0.200120</td>\n",
       "      <td>0.813765</td>\n",
       "      <td>0.171660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      1           2           3           4           5   \\\n",
       "count  351.000000  351.0  351.000000  351.000000  351.000000  351.000000   \n",
       "mean     0.891738    0.0    0.641342    0.044372    0.601068    0.115889   \n",
       "std      0.311155    0.0    0.497708    0.441435    0.519862    0.460810   \n",
       "min      0.000000    0.0   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%      1.000000    0.0    0.472135   -0.064735    0.412660   -0.024795   \n",
       "50%      1.000000    0.0    0.871110    0.016310    0.809200    0.022800   \n",
       "75%      1.000000    0.0    1.000000    0.194185    1.000000    0.334655   \n",
       "max      1.000000    0.0    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               6           7           8           9      ...              24  \\\n",
       "count  351.000000  351.000000  351.000000  351.000000     ...      351.000000   \n",
       "mean     0.550095    0.119360    0.511848    0.181345     ...        0.396135   \n",
       "std      0.492654    0.520750    0.507066    0.483851     ...        0.578451   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000     ...       -1.000000   \n",
       "25%      0.211310   -0.054840    0.087110   -0.048075     ...        0.000000   \n",
       "50%      0.728730    0.014710    0.684210    0.018290     ...        0.553890   \n",
       "75%      0.969240    0.445675    0.953240    0.534195     ...        0.905240   \n",
       "max      1.000000    1.000000    1.000000    1.000000     ...        1.000000   \n",
       "\n",
       "               25          26          27          28          29          30  \\\n",
       "count  351.000000  351.000000  351.000000  351.000000  351.000000  351.000000   \n",
       "mean    -0.071187    0.541641   -0.069538    0.378445   -0.027907    0.352514   \n",
       "std      0.508495    0.516205    0.550025    0.575886    0.507974    0.571483   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%     -0.332390    0.286435   -0.443165    0.000000   -0.236885    0.000000   \n",
       "50%     -0.015050    0.708240   -0.017690    0.496640    0.000000    0.442770   \n",
       "75%      0.156765    0.999945    0.153535    0.883465    0.154075    0.857620   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               31          32          33  \n",
       "count  351.000000  351.000000  351.000000  \n",
       "mean    -0.003794    0.349364    0.014480  \n",
       "std      0.513574    0.522663    0.468337  \n",
       "min     -1.000000   -1.000000   -1.000000  \n",
       "25%     -0.242595    0.000000   -0.165350  \n",
       "50%      0.000000    0.409560    0.000000  \n",
       "75%      0.200120    0.813765    0.171660  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data is normalized and serialized into a vector. Make a train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_array = np.array(df_x.as_matrix())\n",
    "y_array = np.array(pd.DataFrame(df_y).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_array[:301]\n",
    "y_train = y_array[:301]\n",
    "\n",
    "x_val = x_array[301:]\n",
    "y_val = y_array[301:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Hidden Layer Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 301 samples, validate on 50 samples\n",
      "Epoch 1/150\n",
      "301/301 [==============================] - 3s 9ms/step - loss: 0.3849 - val_loss: 0.3751\n",
      "Epoch 2/150\n",
      "301/301 [==============================] - 0s 606us/step - loss: 0.3657 - val_loss: 0.3559\n",
      "Epoch 3/150\n",
      "301/301 [==============================] - 0s 632us/step - loss: 0.3474 - val_loss: 0.3370\n",
      "Epoch 4/150\n",
      "301/301 [==============================] - 0s 601us/step - loss: 0.3292 - val_loss: 0.3196\n",
      "Epoch 5/150\n",
      "301/301 [==============================] - 0s 732us/step - loss: 0.3133 - val_loss: 0.3044\n",
      "Epoch 6/150\n",
      "301/301 [==============================] - 0s 621us/step - loss: 0.3002 - val_loss: 0.2918\n",
      "Epoch 7/150\n",
      "301/301 [==============================] - 0s 634us/step - loss: 0.2870 - val_loss: 0.2818\n",
      "Epoch 8/150\n",
      "301/301 [==============================] - 0s 651us/step - loss: 0.2810 - val_loss: 0.2736\n",
      "Epoch 9/150\n",
      "301/301 [==============================] - 0s 659us/step - loss: 0.2724 - val_loss: 0.2666\n",
      "Epoch 10/150\n",
      "301/301 [==============================] - 0s 607us/step - loss: 0.2652 - val_loss: 0.2607\n",
      "Epoch 11/150\n",
      "301/301 [==============================] - 0s 651us/step - loss: 0.2588 - val_loss: 0.2554\n",
      "Epoch 12/150\n",
      "301/301 [==============================] - 0s 639us/step - loss: 0.2543 - val_loss: 0.2505\n",
      "Epoch 13/150\n",
      "301/301 [==============================] - 0s 617us/step - loss: 0.2490 - val_loss: 0.2459\n",
      "Epoch 14/150\n",
      "301/301 [==============================] - 0s 669us/step - loss: 0.2455 - val_loss: 0.2415\n",
      "Epoch 15/150\n",
      "301/301 [==============================] - 0s 639us/step - loss: 0.2376 - val_loss: 0.2374\n",
      "Epoch 16/150\n",
      "301/301 [==============================] - 0s 624us/step - loss: 0.2389 - val_loss: 0.2336\n",
      "Epoch 17/150\n",
      "301/301 [==============================] - 0s 594us/step - loss: 0.2319 - val_loss: 0.2299\n",
      "Epoch 18/150\n",
      "301/301 [==============================] - 0s 652us/step - loss: 0.2281 - val_loss: 0.2264\n",
      "Epoch 19/150\n",
      "301/301 [==============================] - 0s 689us/step - loss: 0.2248 - val_loss: 0.2230\n",
      "Epoch 20/150\n",
      "301/301 [==============================] - 0s 654us/step - loss: 0.2221 - val_loss: 0.2200\n",
      "Epoch 21/150\n",
      "301/301 [==============================] - 0s 654us/step - loss: 0.2173 - val_loss: 0.2173\n",
      "Epoch 22/150\n",
      "301/301 [==============================] - 0s 647us/step - loss: 0.2180 - val_loss: 0.2148\n",
      "Epoch 23/150\n",
      "301/301 [==============================] - 0s 636us/step - loss: 0.2138 - val_loss: 0.2124\n",
      "Epoch 24/150\n",
      "301/301 [==============================] - 0s 646us/step - loss: 0.2119 - val_loss: 0.2103\n",
      "Epoch 25/150\n",
      "301/301 [==============================] - 0s 700us/step - loss: 0.2122 - val_loss: 0.2084\n",
      "Epoch 26/150\n",
      "301/301 [==============================] - 0s 647us/step - loss: 0.2075 - val_loss: 0.2065\n",
      "Epoch 27/150\n",
      "301/301 [==============================] - 0s 636us/step - loss: 0.2079 - val_loss: 0.2048\n",
      "Epoch 28/150\n",
      "301/301 [==============================] - 0s 664us/step - loss: 0.2058 - val_loss: 0.2033\n",
      "Epoch 29/150\n",
      "301/301 [==============================] - 0s 692us/step - loss: 0.2023 - val_loss: 0.2018\n",
      "Epoch 30/150\n",
      "301/301 [==============================] - 0s 631us/step - loss: 0.2004 - val_loss: 0.2004\n",
      "Epoch 31/150\n",
      "301/301 [==============================] - 0s 649us/step - loss: 0.2009 - val_loss: 0.1991\n",
      "Epoch 32/150\n",
      "301/301 [==============================] - 0s 686us/step - loss: 0.1978 - val_loss: 0.1979\n",
      "Epoch 33/150\n",
      "301/301 [==============================] - 0s 769us/step - loss: 0.1985 - val_loss: 0.1969\n",
      "Epoch 34/150\n",
      "301/301 [==============================] - 0s 724us/step - loss: 0.1980 - val_loss: 0.1958\n",
      "Epoch 35/150\n",
      "301/301 [==============================] - 0s 681us/step - loss: 0.1933 - val_loss: 0.1948\n",
      "Epoch 36/150\n",
      "301/301 [==============================] - 0s 627us/step - loss: 0.1926 - val_loss: 0.1939\n",
      "Epoch 37/150\n",
      "301/301 [==============================] - 0s 769us/step - loss: 0.1912 - val_loss: 0.1931\n",
      "Epoch 38/150\n",
      "301/301 [==============================] - 0s 759us/step - loss: 0.1919 - val_loss: 0.1922\n",
      "Epoch 39/150\n",
      "301/301 [==============================] - 0s 724us/step - loss: 0.1898 - val_loss: 0.1914\n",
      "Epoch 40/150\n",
      "301/301 [==============================] - 0s 752us/step - loss: 0.1905 - val_loss: 0.1907\n",
      "Epoch 41/150\n",
      "301/301 [==============================] - 0s 666us/step - loss: 0.1889 - val_loss: 0.1900\n",
      "Epoch 42/150\n",
      "301/301 [==============================] - 0s 707us/step - loss: 0.1888 - val_loss: 0.1893\n",
      "Epoch 43/150\n",
      "301/301 [==============================] - 0s 725us/step - loss: 0.1882 - val_loss: 0.1885\n",
      "Epoch 44/150\n",
      "301/301 [==============================] - 0s 674us/step - loss: 0.1868 - val_loss: 0.1879\n",
      "Epoch 45/150\n",
      "301/301 [==============================] - 0s 667us/step - loss: 0.1873 - val_loss: 0.1873\n",
      "Epoch 46/150\n",
      "301/301 [==============================] - 0s 697us/step - loss: 0.1878 - val_loss: 0.1867\n",
      "Epoch 47/150\n",
      "301/301 [==============================] - 0s 666us/step - loss: 0.1859 - val_loss: 0.1861\n",
      "Epoch 48/150\n",
      "301/301 [==============================] - 0s 767us/step - loss: 0.1843 - val_loss: 0.1854\n",
      "Epoch 49/150\n",
      "301/301 [==============================] - 0s 829us/step - loss: 0.1827 - val_loss: 0.1849\n",
      "Epoch 50/150\n",
      "301/301 [==============================] - 0s 697us/step - loss: 0.1831 - val_loss: 0.1843\n",
      "Epoch 51/150\n",
      "301/301 [==============================] - 0s 719us/step - loss: 0.1828 - val_loss: 0.1839\n",
      "Epoch 52/150\n",
      "301/301 [==============================] - 0s 617us/step - loss: 0.1797 - val_loss: 0.1833\n",
      "Epoch 53/150\n",
      "301/301 [==============================] - 0s 679us/step - loss: 0.1808 - val_loss: 0.1828\n",
      "Epoch 54/150\n",
      "301/301 [==============================] - 0s 646us/step - loss: 0.1801 - val_loss: 0.1824\n",
      "Epoch 55/150\n",
      "301/301 [==============================] - 0s 626us/step - loss: 0.1813 - val_loss: 0.1819\n",
      "Epoch 56/150\n",
      "301/301 [==============================] - 0s 634us/step - loss: 0.1812 - val_loss: 0.1815\n",
      "Epoch 57/150\n",
      "301/301 [==============================] - 0s 564us/step - loss: 0.1800 - val_loss: 0.1811\n",
      "Epoch 58/150\n",
      "301/301 [==============================] - 0s 547us/step - loss: 0.1817 - val_loss: 0.1807\n",
      "Epoch 59/150\n",
      "301/301 [==============================] - 0s 599us/step - loss: 0.1816 - val_loss: 0.1803\n",
      "Epoch 60/150\n",
      "301/301 [==============================] - 0s 715us/step - loss: 0.1796 - val_loss: 0.1799\n",
      "Epoch 61/150\n",
      "301/301 [==============================] - 0s 700us/step - loss: 0.1796 - val_loss: 0.1795\n",
      "Epoch 62/150\n",
      "301/301 [==============================] - 0s 564us/step - loss: 0.1804 - val_loss: 0.1792\n",
      "Epoch 63/150\n",
      "301/301 [==============================] - 0s 720us/step - loss: 0.1769 - val_loss: 0.1788\n",
      "Epoch 64/150\n",
      "301/301 [==============================] - 0s 519us/step - loss: 0.1758 - val_loss: 0.1784\n",
      "Epoch 65/150\n",
      "301/301 [==============================] - 0s 582us/step - loss: 0.1788 - val_loss: 0.1780\n",
      "Epoch 66/150\n",
      "301/301 [==============================] - 0s 579us/step - loss: 0.1777 - val_loss: 0.1776\n",
      "Epoch 67/150\n",
      "301/301 [==============================] - 0s 737us/step - loss: 0.1769 - val_loss: 0.1773\n",
      "Epoch 68/150\n",
      "301/301 [==============================] - 0s 629us/step - loss: 0.1756 - val_loss: 0.1769\n",
      "Epoch 69/150\n",
      "301/301 [==============================] - 0s 621us/step - loss: 0.1778 - val_loss: 0.1766\n",
      "Epoch 70/150\n",
      "301/301 [==============================] - 0s 622us/step - loss: 0.1736 - val_loss: 0.1762\n",
      "Epoch 71/150\n",
      "301/301 [==============================] - 0s 825us/step - loss: 0.1744 - val_loss: 0.1759\n",
      "Epoch 72/150\n",
      "301/301 [==============================] - 0s 662us/step - loss: 0.1741 - val_loss: 0.1756\n",
      "Epoch 73/150\n",
      "301/301 [==============================] - 0s 709us/step - loss: 0.1746 - val_loss: 0.1752\n",
      "Epoch 74/150\n",
      "301/301 [==============================] - 0s 629us/step - loss: 0.1737 - val_loss: 0.1750\n",
      "Epoch 75/150\n",
      "301/301 [==============================] - 0s 764us/step - loss: 0.1751 - val_loss: 0.1747\n",
      "Epoch 76/150\n",
      "301/301 [==============================] - 0s 694us/step - loss: 0.1735 - val_loss: 0.1745\n",
      "Epoch 77/150\n",
      "301/301 [==============================] - 0s 682us/step - loss: 0.1722 - val_loss: 0.1742\n",
      "Epoch 78/150\n",
      "301/301 [==============================] - 0s 654us/step - loss: 0.1714 - val_loss: 0.1739\n",
      "Epoch 79/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 0s 606us/step - loss: 0.1736 - val_loss: 0.1736\n",
      "Epoch 80/150\n",
      "301/301 [==============================] - 0s 517us/step - loss: 0.1747 - val_loss: 0.1733\n",
      "Epoch 81/150\n",
      "301/301 [==============================] - 0s 722us/step - loss: 0.1744 - val_loss: 0.1732\n",
      "Epoch 82/150\n",
      "301/301 [==============================] - 0s 519us/step - loss: 0.1735 - val_loss: 0.1729\n",
      "Epoch 83/150\n",
      "301/301 [==============================] - 0s 542us/step - loss: 0.1734 - val_loss: 0.1727\n",
      "Epoch 84/150\n",
      "301/301 [==============================] - 0s 651us/step - loss: 0.1706 - val_loss: 0.1724\n",
      "Epoch 85/150\n",
      "301/301 [==============================] - 0s 531us/step - loss: 0.1724 - val_loss: 0.1722\n",
      "Epoch 86/150\n",
      "301/301 [==============================] - 0s 647us/step - loss: 0.1702 - val_loss: 0.1720\n",
      "Epoch 87/150\n",
      "301/301 [==============================] - 0s 517us/step - loss: 0.1709 - val_loss: 0.1718\n",
      "Epoch 88/150\n",
      "301/301 [==============================] - 0s 717us/step - loss: 0.1698 - val_loss: 0.1715\n",
      "Epoch 89/150\n",
      "301/301 [==============================] - 0s 646us/step - loss: 0.1715 - val_loss: 0.1713\n",
      "Epoch 90/150\n",
      "301/301 [==============================] - 0s 719us/step - loss: 0.1681 - val_loss: 0.1711\n",
      "Epoch 91/150\n",
      "301/301 [==============================] - 0s 709us/step - loss: 0.1692 - val_loss: 0.1708\n",
      "Epoch 92/150\n",
      "301/301 [==============================] - 0s 649us/step - loss: 0.1709 - val_loss: 0.1706\n",
      "Epoch 93/150\n",
      "301/301 [==============================] - 0s 649us/step - loss: 0.1691 - val_loss: 0.1705\n",
      "Epoch 94/150\n",
      "301/301 [==============================] - 0s 782us/step - loss: 0.1688 - val_loss: 0.1703\n",
      "Epoch 95/150\n",
      "301/301 [==============================] - 0s 639us/step - loss: 0.1694 - val_loss: 0.1700\n",
      "Epoch 96/150\n",
      "301/301 [==============================] - 0s 672us/step - loss: 0.1696 - val_loss: 0.1699\n",
      "Epoch 97/150\n",
      "301/301 [==============================] - 0s 699us/step - loss: 0.1675 - val_loss: 0.1696\n",
      "Epoch 98/150\n",
      "301/301 [==============================] - 0s 707us/step - loss: 0.1697 - val_loss: 0.1694\n",
      "Epoch 99/150\n",
      "301/301 [==============================] - 0s 579us/step - loss: 0.1663 - val_loss: 0.1692\n",
      "Epoch 100/150\n",
      "301/301 [==============================] - 0s 651us/step - loss: 0.1716 - val_loss: 0.1691\n",
      "Epoch 101/150\n",
      "301/301 [==============================] - 0s 679us/step - loss: 0.1676 - val_loss: 0.1690\n",
      "Epoch 102/150\n",
      "301/301 [==============================] - 0s 612us/step - loss: 0.1660 - val_loss: 0.1687\n",
      "Epoch 103/150\n",
      "301/301 [==============================] - 0s 647us/step - loss: 0.1688 - val_loss: 0.1685\n",
      "Epoch 104/150\n",
      "301/301 [==============================] - 0s 626us/step - loss: 0.1673 - val_loss: 0.1683\n",
      "Epoch 105/150\n",
      "301/301 [==============================] - 0s 604us/step - loss: 0.1687 - val_loss: 0.1682\n",
      "Epoch 106/150\n",
      "301/301 [==============================] - 0s 639us/step - loss: 0.1640 - val_loss: 0.1680\n",
      "Epoch 107/150\n",
      "301/301 [==============================] - 0s 662us/step - loss: 0.1678 - val_loss: 0.1679\n",
      "Epoch 108/150\n",
      "301/301 [==============================] - 0s 682us/step - loss: 0.1646 - val_loss: 0.1677\n",
      "Epoch 109/150\n",
      "301/301 [==============================] - 0s 667us/step - loss: 0.1663 - val_loss: 0.1676\n",
      "Epoch 110/150\n",
      "301/301 [==============================] - 0s 686us/step - loss: 0.1668 - val_loss: 0.1674\n",
      "Epoch 111/150\n",
      "301/301 [==============================] - 0s 672us/step - loss: 0.1671 - val_loss: 0.1673\n",
      "Epoch 112/150\n",
      "301/301 [==============================] - 0s 662us/step - loss: 0.1672 - val_loss: 0.1672\n",
      "Epoch 113/150\n",
      "301/301 [==============================] - 0s 729us/step - loss: 0.1678 - val_loss: 0.1670\n",
      "Epoch 114/150\n",
      "301/301 [==============================] - 0s 714us/step - loss: 0.1671 - val_loss: 0.1669\n",
      "Epoch 115/150\n",
      "301/301 [==============================] - 0s 730us/step - loss: 0.1634 - val_loss: 0.1667\n",
      "Epoch 116/150\n",
      "301/301 [==============================] - 0s 671us/step - loss: 0.1667 - val_loss: 0.1666\n",
      "Epoch 117/150\n",
      "301/301 [==============================] - 0s 717us/step - loss: 0.1646 - val_loss: 0.1666\n",
      "Epoch 118/150\n",
      "301/301 [==============================] - 0s 617us/step - loss: 0.1636 - val_loss: 0.1664\n",
      "Epoch 119/150\n",
      "301/301 [==============================] - 0s 694us/step - loss: 0.1661 - val_loss: 0.1662\n",
      "Epoch 120/150\n",
      "301/301 [==============================] - 0s 725us/step - loss: 0.1655 - val_loss: 0.1661\n",
      "Epoch 121/150\n",
      "301/301 [==============================] - 0s 819us/step - loss: 0.1630 - val_loss: 0.1660\n",
      "Epoch 122/150\n",
      "301/301 [==============================] - ETA: 0s - loss: 0.159 - 0s 671us/step - loss: 0.1631 - val_loss: 0.1658\n",
      "Epoch 123/150\n",
      "301/301 [==============================] - 0s 735us/step - loss: 0.1626 - val_loss: 0.1657\n",
      "Epoch 124/150\n",
      "301/301 [==============================] - 0s 592us/step - loss: 0.1613 - val_loss: 0.1656\n",
      "Epoch 125/150\n",
      "301/301 [==============================] - 0s 705us/step - loss: 0.1638 - val_loss: 0.1654\n",
      "Epoch 126/150\n",
      "301/301 [==============================] - 0s 636us/step - loss: 0.1660 - val_loss: 0.1653\n",
      "Epoch 127/150\n",
      "301/301 [==============================] - 0s 767us/step - loss: 0.1647 - val_loss: 0.1652\n",
      "Epoch 128/150\n",
      "301/301 [==============================] - 0s 710us/step - loss: 0.1654 - val_loss: 0.1651\n",
      "Epoch 129/150\n",
      "301/301 [==============================] - 0s 564us/step - loss: 0.1662 - val_loss: 0.1650\n",
      "Epoch 130/150\n",
      "301/301 [==============================] - 0s 686us/step - loss: 0.1606 - val_loss: 0.1649\n",
      "Epoch 131/150\n",
      "301/301 [==============================] - 0s 710us/step - loss: 0.1617 - val_loss: 0.1648\n",
      "Epoch 132/150\n",
      "301/301 [==============================] - 0s 639us/step - loss: 0.1621 - val_loss: 0.1646\n",
      "Epoch 133/150\n",
      "301/301 [==============================] - 0s 707us/step - loss: 0.1626 - val_loss: 0.1646\n",
      "Epoch 134/150\n",
      "301/301 [==============================] - 0s 672us/step - loss: 0.1628 - val_loss: 0.1644\n",
      "Epoch 135/150\n",
      "301/301 [==============================] - 0s 759us/step - loss: 0.1666 - val_loss: 0.1644\n",
      "Epoch 136/150\n",
      "301/301 [==============================] - 0s 659us/step - loss: 0.1616 - val_loss: 0.1643\n",
      "Epoch 137/150\n",
      "301/301 [==============================] - 0s 694us/step - loss: 0.1637 - val_loss: 0.1642\n",
      "Epoch 138/150\n",
      "301/301 [==============================] - 0s 639us/step - loss: 0.1631 - val_loss: 0.1641\n",
      "Epoch 139/150\n",
      "301/301 [==============================] - 0s 674us/step - loss: 0.1638 - val_loss: 0.1640\n",
      "Epoch 140/150\n",
      "301/301 [==============================] - 0s 599us/step - loss: 0.1623 - val_loss: 0.1639\n",
      "Epoch 141/150\n",
      "301/301 [==============================] - 0s 714us/step - loss: 0.1631 - val_loss: 0.1639\n",
      "Epoch 142/150\n",
      "301/301 [==============================] - 0s 719us/step - loss: 0.1619 - val_loss: 0.1638\n",
      "Epoch 143/150\n",
      "301/301 [==============================] - 0s 679us/step - loss: 0.1647 - val_loss: 0.1638\n",
      "Epoch 144/150\n",
      "301/301 [==============================] - 0s 690us/step - loss: 0.1597 - val_loss: 0.1636\n",
      "Epoch 145/150\n",
      "301/301 [==============================] - 0s 681us/step - loss: 0.1635 - val_loss: 0.1636\n",
      "Epoch 146/150\n",
      "301/301 [==============================] - 0s 619us/step - loss: 0.1638 - val_loss: 0.1635\n",
      "Epoch 147/150\n",
      "301/301 [==============================] - 0s 775us/step - loss: 0.1593 - val_loss: 0.1634\n",
      "Epoch 148/150\n",
      "301/301 [==============================] - 0s 707us/step - loss: 0.1608 - val_loss: 0.1633\n",
      "Epoch 149/150\n",
      "301/301 [==============================] - 0s 609us/step - loss: 0.1614 - val_loss: 0.1632\n",
      "Epoch 150/150\n",
      "301/301 [==============================] - 0s 651us/step - loss: 0.1595 - val_loss: 0.1631\n",
      "50/50 [==============================] - 0s 200us/step\n",
      "0.163072003126\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(input_dim, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adadelta')\n",
    "\n",
    "model.fit(x_train, x_train, \n",
    "          epochs=150,\n",
    "          shuffle=True,\n",
    "          batch_size=4,\n",
    "          verbose=0,\n",
    "          validation_data=(x_val, x_val)\n",
    "         )\n",
    "score = model.evaluate(x_val, x_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does a classifier after the vanilla autoencoder perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 70us/step\n",
      "[0.065327906906604769, 0.97999999046325681]\n"
     ]
    }
   ],
   "source": [
    "model1_exp = Sequential()\n",
    "model1_exp.add(Dense(1, input_dim=input_dim, activation='sigmoid'))\n",
    "\n",
    "model1_exp.compile(loss='binary_crossentropy',\n",
    "                   optimizer='adadelta',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model1_exp.fit(x_train, y_train, \n",
    "              epochs=150,\n",
    "              shuffle=False,\n",
    "              batch_size=4,\n",
    "              verbose=0,\n",
    "              validation_data=(x_val, y_val)\n",
    "             )\n",
    "score = model.evaluate(x_val, y_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## constants for stacked autoencoder ############\n",
    "encoding_dim1 = 16\n",
    "encoding_dim2 = 8\n",
    "decoding_dim1 = 16\n",
    "decoding_dim2 = input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 70us/step\n",
      "0.1751718238\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(encoding_dim1, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(encoding_dim2, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(decoding_dim1, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(decoding_dim2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adadelta')\n",
    "\n",
    "model.fit(x_train, x_train, \n",
    "          epochs=150,\n",
    "          shuffle=True,\n",
    "          batch_size=4,\n",
    "          verbose=0,\n",
    "          validation_data=(x_val, x_val)\n",
    "         )\n",
    "score = model.evaluate(x_val, x_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 90us/step\n",
      "[0.21874655961990355, 0.89999999523162844]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(decoding_dim1, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          epochs=150,\n",
    "          shuffle=False,\n",
    "          batch_size=4,\n",
    "          verbose=0,\n",
    "          validation_data=(x_val, y_val)\n",
    "         )\n",
    "score = model.evaluate(x_val, y_val)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
